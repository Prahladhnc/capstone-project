{"instruction": "Generate a question for Unit 1: Deep Feedforward Networks. Topics include: ReLU and Tanh Activation Functions, Saturation.", "input": "", "output": "Illustrate how ReLU and Tanh activation functions exhibit saturation. Explain how saturation affects the learning process and the gradients."}
{"instruction": "Generate a question for Unit 1: Deep Feedforward Networks. Topics include: ReLU Activation, Softmax Output Layer, DNN Structure, and Output Probabilities.", "input": "", "output": "Consider a Deep Neural Network (DNN) with the following structure:\n‚Ä¢ Inputs to the hidden layer: z = [0.8, 0.6, 0.7, 0.9].\n‚Ä¢ Weight matrix connecting the hidden layer to the output layer:\n‚Ä¢ Bias vector for the output layer: [0.1, 0.3, 0.5].\n‚Ä¢ Activation function at the hidden layer: ReLU\n‚Ä¢ Activation function at the output layer: Softmax\nCompute the output probabilities for each class."}
{"instruction": "Generate a question for Unit 1: Deep Feedforward Networks. Topics include: Heuristics for Faster Training of DNNs.", "input": "", "output": "Describe any two heuristics that can be used to accelerate the training process of a Deep Neural Network (DNN). Discuss their significance in reducing training time and improving model performance."}
{"instruction": "Generate a question for Unit 1: Deep Feedforward Networks. Topics include: Injecting Noise at Output Targets.", "input": "", "output": "Describe the significance of injecting noise at the output targets during the training of a neural network. How does this technique improve the model's robustness?"}
{"instruction": "Generate a question for Unit 1: Training Deep Neural Networks. Topics include: Cross-Entropy Loss Function, Binary Classification, Neural Network Error Computation.", "input": "", "output": "Consider a binary classification problem where a neural network predicts whether an image contains a cat (class 1) or not (class 0). True labels are y = [1, 0, 1, 1, 0] and the predicted probabilities for class 1 are p = [0.9, 0.3, 0.8, 0.88, 0.1]. Apply the cross-entropy loss function and compute the error between the predicted probabilities and the true labels."}
{"instruction": "Generate a question for Unit 1: Training Deep Neural Networks. Topics include: Data Augmentation.", "input": "", "output": "Explain the need for data augmentation in training Deep Neural Networks. How does data augmentation improve generalization and prevent overfitting?"}
{"instruction": "Generate a question for Unit 2: Training Deep Neural Networks. Topics include: Early Stopping Algorithm.", "input": "", "output": "Write an early stopping meta-algorithm to determine the best amount of training time. Explain how the algorithm works and how it helps in preventing overfitting during training."}
{"instruction": "Generate a question for Unit 2: Training Deep Neural Networks. Topics include: Gradient Clipping and Exploding Gradient Problem.", "input": "", "output": "How does gradient clipping help in addressing the exploding gradient problem? Explain the concept and its impact on model training, especially in deep neural networks."}
{"instruction": "Generate a question for Unit 2: Training Deep Neural Networks. Topics include: Densely Connected Base Network and Subnetworks.", "input": "", "output": "Consider a densely connected base network featuring two visible units, one hidden layer containing two hidden units, and a single output unit. Illustrate the base network and depict all possible subnetworks that can be generated by dropping out various subsets of units from the base network."}
{"instruction": "Generate a question for Unit 2: Training Deep Neural Networks. Topics include: Parameter Tying and Parameter Sharing.", "input": "", "output": "Explain parameter tying and parameter sharing using a suitable application. Discuss their significance in reducing the number of parameters and improving the efficiency of the model."}
{"instruction": "Generate a question for Unit 2: Training Deep Neural Networks. Topics include: RMSProp and RMSProp with Nesterov Momentum.", "input": "", "output": "Differentiate RMSProp and RMSProp with Nesterov momentum. Explain their working principles, advantages, and situations where each is preferred."}
{"instruction": "Generate a question for Unit 2: Training Deep Neural Networks. Topics include: Early Stopping as a Meta-Algorithm.", "input": "", "output": "Explain the use of early stopping as a meta-algorithm to: \ni. Identify the onset of overfitting during training. \nii. Determine the optimal number of training steps for retraining on the full dataset."}
{"instruction": "Generate a question for Unit 3: Recurrent Neural Networks (RNN). Topics include: LSTM and GRU differentiation.", "input": "", "output": "Using suitable mathematical equations, differentiate between Long Short Term Memory (LSTM) and Gated Recurrent Units (GRUs). Discuss the key differences in terms of their architectures and operations."}
{"instruction": "Generate a question for Unit 3: Recurrent Neural Networks (RNN). Topics include: LSTM cell computations.", "input": "", "output": "Compute the values of the input gate and output gate of an LSTM cell by considering the following data: \n- Weights for the input gate \n- Hidden state \n- Recurrent weights for the input gate \n- Biases for the input gate \n- Input vector \n- Weights for the output gate \n- Recurrent weights for the output gate \n- Biases for the output gate."}
{"instruction": "Generate a question for Unit 3: Recurrent Neural Networks (RNN). Topics include: Vanishing Gradient Problem.", "input": "", "output": "Explain the vanishing gradient problem in Recurrent Neural Networks (RNN). Discuss its impact on training deep networks and techniques to mitigate it."}
{"instruction": "Generate a question for Unit 3: Recurrent Neural Networks (RNN). Topics include: Vanilla RNN vs Bi-directional RNN.", "input": "", "output": "How does Vanilla RNN differ from Bi-directional RNN? Compare their architectures and explain the advantages of using Bi-directional RNN."}
{"instruction": "Generate a question for Unit 3: Recurrent Neural Networks (RNN). Topics include: GRU cell computations.", "input": "", "output": "Compute the values of the update gate and the Candidate Hidden State of a GRU cell by considering the given data: \n- Weights for the update gate \n- Biases for the update gate \n- Input vector \n- Previous hidden state \n- Candidate Activation \n- Reset Gate."}
{"instruction": "Generate a question for Unit 3: Recurrent Neural Networks (RNN). Topics include: Back Propagation Through Time (BPTT) algorithm.", "input": "", "output": "Write the Back Propagation Through Time (BPTT) algorithm. Explain how gradients are propagated through time in RNNs and how it helps in updating the weights during training."}
{"instruction": "Generate a question for Unit 4: Convolutional Neural Networks (CNN). Topics include: Convolution, ReLU activation, and Max pooling.", "input": "", "output": "Consider a Convolutional Neural Network (CNN) with the following configuration: \nAn input image of size 4x4 and a single 2x2 filter (kernel). \nPerform the following operations: \n i. Perform the convolution operation on the given input image using the specified filter and stride = 1. Write the resulting feature map. \n ii. Apply the ReLU activation function to the result obtained in (i). Represent the updated feature map in matrix form. \n iii. Perform max pooling on the feature map obtained from (ii) using a 2x2 pooling window and stride = 1. Determine the resulting feature map."}
{"instruction": "Generate a question for Unit 4: Convolutional Neural Networks (CNN). Topics include: Retaining original size after convolution.", "input": "", "output": "Is it possible to retain the original size of the image after the process of convolution? Justify with an example."}
{"instruction": "Generate a question for Unit 4: Convolutional Neural Networks (CNN). Topics include: Adversarial training applications.", "input": "", "output": "Elaborate on key application areas of adversarial training. Discuss how it is used to improve the robustness and generalization of deep learning models."}
{"instruction": "Generate a question for Unit 4: Convolutional Neural Networks (CNN). Topics include: CNN architecture for handwritten digit classification.", "input": "", "output": "Design a CNN with the following specifications to classify images of handwritten digits between 0 and 9. Represent the size of the intermediate feature maps precisely on the architectural design. \n‚Ä¢ Input: (28, 28, 1) \n‚Ä¢ Conv1: 32 filters, kernel: 3x3, activation function: ReLU \n‚Ä¢ Pool1: 2x2 Max Pooling \n‚Ä¢ Conv2: 64 filters, kernel: 3x3, activation function: ReLU \n‚Ä¢ Pool2: 2x2 Max Pooling \n‚Ä¢ Flatten \n‚Ä¢ Dense: 128 neurons, activation function: ReLU \n‚Ä¢ Output: 10 neurons, activation function: Softmax."}
{"instruction": "Generate a question for Unit 4: Convolutional Neural Networks (CNN). Topics include: Calculating feature map size with a kernel.", "input": "", "output": "Given an image of size 30x30 with a 3x3 kernel, horizontal stride = 2, and vertical stride = 2. Calculate the size of the resultant feature map."}
{"instruction": "Generate a question for Unit 4: Convolutional Neural Networks (CNN). Topics include: Manifold Tangent Classifier for improving classification accuracy.", "input": "", "output": "Can Manifold Tangent Classifier be used to improve the accuracy of classification? Justify your answer with reasoning and examples."}
{"instruction": "Generate a question for Unit 5: Restricted Boltzmann Machines (RBM). Topics include: Training a Hopfield Network.", "input": "", "output": "Explain the process of training a Hopfield network along with relevant mathematical representations."}
{"instruction": "Generate a question for Unit 5: Restricted Boltzmann Machines (RBM). Topics include: Contrastive Divergence algorithm.", "input": "", "output": "Illustrate the steps of the Contrastive Divergence algorithm for training a Restricted Boltzmann Machine (RBM)."}
{"instruction": "Generate a question for Unit 5: Restricted Boltzmann Machines (RBM). Topics include: Calculating hidden unit probabilities.", "input": "", "output": "Consider an RBM with 2 visible units and 2 hidden units. Given the weights and biases: \n- Weight matrix \n- Visible unit biases \n- Hidden unit biases \nCalculate the probabilities of the hidden units being activated given a specific visible vector v=[1,0]."}
{"instruction": "Generate a question for Unit 5: Restricted Boltzmann Machines (RBM). Topics include: Training multilayer RBM.", "input": "", "output": "Illustrate the training process of multilayer Restricted Boltzmann Machines."}
{"instruction": "Generate a question for Unit 5: Restricted Boltzmann Machines (RBM). Topics include: Boltzmann Machine vs Restricted Boltzmann Machine.", "input": "", "output": "Differentiate between Boltzmann Machine and Restricted Boltzmann Machine."}
{"instruction": "Generate a question for Unit 5: Restricted Boltzmann Machines (RBM). Topics include: Weight updates using Contrastive Divergence algorithm.", "input": "", "output": "A Restricted Boltzmann Machine (RBM) with 3 visible units and 2 hidden units has the following initial weights ùëä between the visible and hidden units: \nThe biases for the visible units and the hidden units are: \nGiven an input vector ùë£ = (1 0 1) and learning rate = 0.1, use the sigmoid function and illustrate the process of weight updates using the Contrastive Divergence algorithm for 1 epoch."}
{"instruction": "Generate a question for Unit 1: Deep Feedforward Networks. Topics include: Gradient-Based Learning, Hidden Units, Architecture, Unit saturation, vanishing gradient, ways to mitigate it, ReLU, Heuristics for avoiding bad local minima, Heuristics for faster training, Nesterov accelerated gradient descent, Dataset Augmentation, Noise Robustness.", "input": "", "output": "Solve the XOR problem using a Feedforward Neural Network. Provide the architecture, activation functions, and detailed learning process along with necessary equations to demonstrate how the network can learn a non-linearly separable function."}
{"instruction": "Generate a question for Unit 1: Deep Feedforward Networks. Topics include: Gradient-Based Learning, Hidden Units, Architecture, Unit saturation, vanishing gradient, ways to mitigate it, ReLU, Heuristics for avoiding bad local minima, Heuristics for faster training, Nesterov accelerated gradient descent, Dataset Augmentation, Noise Robustness.", "input": "", "output": "What is the significance of Early Stopping in training deep neural networks? Rewrite the Early Stopping meta-algorithm for determining the best training duration to avoid overfitting."}
{"instruction": "Generate a question for Unit 1: Deep Feedforward Networks. Topics include: Gradient-Based Learning, Hidden Units, Architecture, Unit saturation, vanishing gradient, ways to mitigate it, ReLU, Heuristics for avoiding bad local minima, Heuristics for faster training, Nesterov accelerated gradient descent, Dataset Augmentation, Noise Robustness.", "input": "", "output": "Explain how the choice of cost function plays a critical role in designing a Deep Neural Network. Discuss how it affects the training dynamics and final model performance."}
{"instruction": "Generate a question for Unit 1: Deep Feedforward Networks. Topics include: Gradient-Based Learning, Hidden Units, Architecture, Unit saturation, vanishing gradient, ways to mitigate it, ReLU, Heuristics for avoiding bad local minima, Heuristics for faster training, Nesterov accelerated gradient descent, Dataset Augmentation, Noise Robustness.", "input": "", "output": "Explain the strategy of combining multiple models (ensemble methods) to reduce generalization error in deep learning. Illustrate with examples how this improves model robustness."}
{"instruction": "Generate a question for Unit 3: Recurrent Neural Networks (RNN). Topics include: Introduction, Expressiveness, Architecture, Language Modeling, Bidirectional RNN, Encoder Decoder architectures, Multilayer RNN, Exploding gradient problem, Challenges of Training RNNs - Layer Normalization, RRBF, LSTM, GRUs.", "input": "", "output": "Compute the gradient in a Recurrent Neural Network (RNN) and describe the backpropagation through time (BPTT) process with relevant mathematical derivations and equations."}
{"instruction": "Generate a question for Unit 3: Recurrent Neural Networks (RNN). Topics include: Introduction, Expressiveness, Architecture, Language Modeling, Bidirectional RNN, Encoder Decoder architectures, Multilayer RNN, Exploding gradient problem, Challenges of Training RNNs - Layer Normalization, RRBF, LSTM, GRUs.", "input": "", "output": "Explain how Bidirectional Recurrent Neural Networks (Bidirectional RNNs) enhance the prediction of output by utilizing both past and future context from the entire input sequence."}
{"instruction": "Generate a question for Unit 4: Convolutional Networks. Topics include: Convolution Operation, Motivation, Pooling, Convolution and Pooling as an Infinitely Strong Prior, Variants of the basic convolution function, Structured Outputs, Data types, Efficient Convolution Algorithms, Random or Unsupervised features, Adversarial Training, Tangent Distance, Tangent Prop and Manifold Tangent Classifier.", "input": "", "output": "Elaborate on the different layers of a Convolutional Neural Network (CNN), including convolutional layers, pooling layers, fully connected layers, and activation layers. Provide necessary diagrams and explain the derivatives used in backpropagation."}
{"instruction": "Generate a question for Unit 4: Convolutional Networks. Topics include: Convolution Operation, Motivation, Pooling, Convolution and Pooling as an Infinitely Strong Prior, Variants of the basic convolution function, Structured Outputs, Data types, Efficient Convolution Algorithms, Random or Unsupervised features, Adversarial Training, Tangent Distance, Tangent Prop and Manifold Tangent Classifier.", "input": "", "output": "List and explain the different data channels that could be used in a Convolutional Neural Network (CNN), such as RGB, grayscale, and depth channels."}
{"instruction": "Generate a question for Unit 4: Convolutional Networks. Topics include: Convolution Operation, Motivation, Pooling, Convolution and Pooling as an Infinitely Strong Prior, Variants of the basic convolution function, Structured Outputs, Data types, Efficient Convolution Algorithms, Random or Unsupervised features, Adversarial Training, Tangent Distance, Tangent Prop and Manifold Tangent Classifier.", "input": "", "output": "What is the importance of pre-trained models in Convolutional Neural Networks (CNNs)? Do they enhance model performance? Explain with examples of popular pre-trained models like VGG, ResNet, or Inception."}
{"instruction": "Generate a question for Unit 5: Restricted Boltzmann Machines. Topics include: Deep Boltzmann Machines (DBM), Variational Stochastic Maximum Likelihood Algorithm, Auto Regressive Networks.", "input": "", "output": "Deduce the Variational Stochastic Maximum Likelihood Algorithm for training a Deep Boltzmann Machine (DBM) with two hidden layers. Discuss the steps and provide the mathematical derivations involved in training the DBM."}
{"instruction": "Generate a question for Unit 5: Restricted Boltzmann Machines. Topics include: Deep Boltzmann Machines (DBM), Variational Stochastic Maximum Likelihood Algorithm, Auto Regressive Networks.", "input": "", "output": "Elaborate on the different types of Auto Regressive Networks. Explain their architectures, working principles, and applications."}
{"instruction": "Generate a question for Unit 5: Restricted Boltzmann Machines. Topics include: Deep Boltzmann Machines (DBM), Variational Stochastic Maximum Likelihood Algorithm, Auto Regressive Networks.", "input": "", "output": "Illustrate the discrete stochastic operations through which you can realize Back-Propagation. Provide the steps involved and the associated mathematical representations."}
{"instruction": "Generate a question for Unit 5: Restricted Boltzmann Machines. Topics include: Deep Boltzmann Machines (DBM), Variational Stochastic Maximum Likelihood Algorithm, Auto Regressive Networks.", "input": "", "output": "Give inferences to the fact that DBM Parameter Learning is an interesting property of DBM. Explain how parameter learning in DBMs contributes to their ability to model complex dependencies and represent high-dimensional data."}